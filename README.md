# ğŸ§  Sentiment Analyzer using Mistral (via Ollama)

## ğŸ“Œ Overview

A simple AI-powered app that analyzes the **sentiment** of any text you enter using the **Mistral** language model via **Ollama**, with:

- ğŸ§  **Mistral** via Ollama â€“ runs locally, no API keys needed
- âš™ï¸ **FastAPI** â€“ handles the backend and model interaction
- ğŸ–¼ï¸ **Streamlit** â€“ provides a simple user interface for input/output

---

## ğŸ§° Tech Stack

- [Ollama](https://ollama.com) â€“ Run LLMs locally
- [FastAPI](https://fastapi.tiangolo.com/) â€“ Modern Python web framework
- [Streamlit](https://streamlit.io) â€“ Frontend UI for apps
- Python 3.10+ with `venv`

---

## âš™ï¸ Setup Instructions

### ğŸ”¹ Step 1: Set up the environment

1. Create and activate a virtual environment:

```bash
python -m venv venv
.\venv\Scripts\activate  # Windows
# or
source venv/bin/activate  # macOS/Linux
```

2. Install the dependencies:

```bash
pip install fastapi uvicorn streamlit requests python-multipart
```

3. Save your installed packages:

```bash
pip freeze > requirements.txt
```

---

### ğŸ”¹ Step 2: Install and run Ollama

1. Download and install Ollama: [https://ollama.com](https://ollama.com)  
2. Pull the Mistral model:

```bash
ollama pull mistral
```

3. Start the model:

```bash
ollama run mistral
```

Keep this running in a terminal window.

---

### ğŸ”¹ Step 3: Run the backend (FastAPI)

1. Navigate to your project root.
2. Start the FastAPI server:

```bash
uvicorn backend.main:app --reload
```

- Visit the auto-generated docs: [http://localhost:8000/docs](http://localhost:8000/docs)

---

### ğŸ”¹ Step 4: Run the frontend (Streamlit)

In a separate terminal (activate `venv` again if needed):

```bash
streamlit run frontend/app.py
```

Visit [http://localhost:8501](http://localhost:8501) to use the app.

---

## ğŸ—‚ï¸ Structure

```
Sentiment-Analyzer using Mistral via Ollama
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ venv/                # local virtual environment (not pushed to GitHub)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Features

- Accepts user text via a web interface
- Sends the text to a locally hosted LLM (Mistral via Ollama)
- Returns a sentiment classification: **Positive**, **Negative**, or **Neutral**

---

## âš ï¸ Notes

- Ensure Ollama is running **before** you start the backend.
- `venv/` is typically excluded using `.gitignore` and **should not be pushed to GitHub**.

## ğŸ“ License

This project is licensed under the [MIT License](LICENSE).

### Acknowledgements

- This project uses [FastAPI](https://fastapi.tiangolo.com/) licensed under the **MIT License**.
- It also uses [Streamlit](https://streamlit.io/) licensed under the **Apache License 2.0**.
- The language model **Mistral 7B** is licensed under the **Apache License 2.0**.
- The model is served locally using [Ollama](https://ollama.com), a proprietary tool.
- Works fully offline once set up.

---



